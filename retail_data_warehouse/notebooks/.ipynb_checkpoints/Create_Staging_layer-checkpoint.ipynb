{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "decf500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import trim,col\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4993fe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Read CSV files\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08973f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "olist_customers_dataset.csv\n",
      "olist_geolocation_dataset.csv\n",
      "olist_orders_dataset.csv\n",
      "olist_order_items_dataset.csv\n",
      "olist_order_payments_dataset.csv\n",
      "olist_order_reviews_dataset.csv\n",
      "olist_products_dataset.csv\n",
      "olist_sellers_dataset.csv\n",
      "product_category_name_translation.csv\n"
     ]
    }
   ],
   "source": [
    "csv_files = list(dataset_path.glob(\"*.csv\"))\n",
    "for file in csv_files:\n",
    "    print(file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3315af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found CSV files: ['olist_customers_dataset.csv', 'olist_geolocation_dataset.csv', 'olist_orders_dataset.csv', 'olist_order_items_dataset.csv', 'olist_order_payments_dataset.csv', 'olist_order_reviews_dataset.csv', 'olist_products_dataset.csv', 'olist_sellers_dataset.csv', 'product_category_name_translation.csv']\n",
      "‚ñ∂Ô∏è Processing olist_customers_dataset.csv ‚Üí customers\n",
      "‚úÖ Written to C:\\Users\\vasudha.tanniru\\Documents\\GitHub\\data_projects\\retail_data_warehouse\\warehouse\\staging\\customers\n",
      "‚ñ∂Ô∏è Processing olist_geolocation_dataset.csv ‚Üí geolocation\n",
      "‚úÖ Written to C:\\Users\\vasudha.tanniru\\Documents\\GitHub\\data_projects\\retail_data_warehouse\\warehouse\\staging\\geolocation\n",
      "‚ñ∂Ô∏è Processing olist_orders_dataset.csv ‚Üí orders\n",
      "‚úÖ Written to C:\\Users\\vasudha.tanniru\\Documents\\GitHub\\data_projects\\retail_data_warehouse\\warehouse\\staging\\orders\n",
      "‚ñ∂Ô∏è Processing olist_order_items_dataset.csv ‚Üí order_items\n",
      "‚úÖ Written to C:\\Users\\vasudha.tanniru\\Documents\\GitHub\\data_projects\\retail_data_warehouse\\warehouse\\staging\\order_items\n",
      "‚ñ∂Ô∏è Processing olist_order_payments_dataset.csv ‚Üí order_payments\n",
      "‚úÖ Written to C:\\Users\\vasudha.tanniru\\Documents\\GitHub\\data_projects\\retail_data_warehouse\\warehouse\\staging\\order_payments\n",
      "‚ñ∂Ô∏è Processing olist_order_reviews_dataset.csv ‚Üí order_reviews\n",
      "‚úÖ Written to C:\\Users\\vasudha.tanniru\\Documents\\GitHub\\data_projects\\retail_data_warehouse\\warehouse\\staging\\order_reviews\n",
      "‚ñ∂Ô∏è Processing olist_products_dataset.csv ‚Üí products\n",
      "‚úÖ Written to C:\\Users\\vasudha.tanniru\\Documents\\GitHub\\data_projects\\retail_data_warehouse\\warehouse\\staging\\products\n",
      "‚ñ∂Ô∏è Processing olist_sellers_dataset.csv ‚Üí sellers\n",
      "‚úÖ Written to C:\\Users\\vasudha.tanniru\\Documents\\GitHub\\data_projects\\retail_data_warehouse\\warehouse\\staging\\sellers\n",
      "‚ñ∂Ô∏è Processing product_category_name_translation.csv ‚Üí product_category_name_translation\n",
      "‚úÖ Written to C:\\Users\\vasudha.tanniru\\Documents\\GitHub\\data_projects\\retail_data_warehouse\\warehouse\\staging\\product_category_name_translation\n",
      "\n",
      "üéâ Staging layer successfully created!\n"
     ]
    }
   ],
   "source": [
    "base_path = r\"C:\\Users\\vasudha.tanniru\\Documents\\GitHub\\data_projects\\retail_data_warehouse\"\n",
    "dataset_path = os.path.join(base_path, \"datasets\")\n",
    "staging_path = os.path.join(base_path, \"warehouse\", \"staging\")\n",
    "\n",
    "\n",
    "csv_files = [f for f in os.listdir(dataset_path) if f.endswith(\".csv\")]\n",
    "print(\"Found CSV files:\", csv_files)\n",
    "\n",
    "def trim_string_columns(df):\n",
    "    string_cols = [f.name for f in df.schema.fields if f.dataType.simpleString() == \"string\"]\n",
    "    for colname in string_cols:\n",
    "        df = df.withColumn(colname, trim(col(colname)))\n",
    "    return df\n",
    "\n",
    "# Loop through all CSVs\n",
    "for file_name in csv_files:\n",
    "    try:\n",
    "        # Derive output folder name (e.g., olist_orders_dataset.csv ‚Üí orders)\n",
    "        table_name = file_name.replace(\"olist_\", \"\").replace(\"_dataset\", \"\").replace(\".csv\", \"\")\n",
    "        table_path = os.path.join(staging_path, table_name)\n",
    "\n",
    "        print(f\"Processing {file_name} ‚Üí {table_name}\")\n",
    "\n",
    "        # Read CSV\n",
    "        df = spark.read.csv(\n",
    "            os.path.join(dataset_path, file_name),\n",
    "            header=True,\n",
    "            inferSchema=True\n",
    "        )\n",
    "\n",
    "        # Clean column names\n",
    "        for old_col in df.columns:\n",
    "            df = df.withColumnRenamed(old_col, old_col.strip().lower())\n",
    "\n",
    "        # Trim string columns\n",
    "        df = trim_string_columns(df)\n",
    "\n",
    "        # Drop duplicates\n",
    "        df = df.dropDuplicates()\n",
    "\n",
    "        # Write to Parquet (single file for readability)\n",
    "        df.coalesce(1).write.mode(\"overwrite\").parquet(table_path)\n",
    "\n",
    "        print(f\"Written to {table_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "print(\"\\n Staging layer successfully created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f0b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (pythonData)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
