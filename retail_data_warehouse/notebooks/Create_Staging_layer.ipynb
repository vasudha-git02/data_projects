{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "decf500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import trim,col\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4993fe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Read CSV files\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08973f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_files = list(dataset_path.glob(\"*.csv\"))\n",
    "# for file in csv_files:\n",
    "#     print(file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3315af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSV files: ['olist_order_reviews_dataset.csv']\n",
      "Processing olist_order_reviews_dataset.csv → order_reviews\n",
      "Written to C:\\Users\\vasudha.tanniru\\Documents\\GitHub\\data_projects\\retail_data_warehouse\\warehouse\\staging\\order_reviews\n",
      "\n",
      " Staging layer successfully created!\n"
     ]
    }
   ],
   "source": [
    "base_path = r\"C:\\Users\\vasudha.tanniru\\Documents\\GitHub\\data_projects\\retail_data_warehouse\"\n",
    "dataset_path = os.path.join(base_path, \"datasets\")\n",
    "staging_path = os.path.join(base_path, \"warehouse\", \"staging\")\n",
    "\n",
    "\n",
    "csv_files = [f for f in os.listdir(dataset_path) if f.endswith(\".csv\")]\n",
    "print(\"Found CSV files:\", csv_files)\n",
    "\n",
    "def trim_string_columns(df):\n",
    "    string_cols = [f.name for f in df.schema.fields if f.dataType.simpleString() == \"string\"]\n",
    "    for colname in string_cols:\n",
    "        df = df.withColumn(colname, trim(col(colname)))\n",
    "    return df\n",
    "\n",
    "# Loop through all CSVs\n",
    "for file_name in csv_files:\n",
    "    try:\n",
    "        # Derive output folder name (e.g., olist_orders_dataset.csv → orders)\n",
    "        table_name = file_name.replace(\"olist_\", \"\").replace(\"_dataset\", \"\").replace(\".csv\", \"\")\n",
    "        table_path = os.path.join(staging_path, table_name)\n",
    "\n",
    "        print(f\"Processing {file_name} → {table_name}\")\n",
    "\n",
    "        # Read CSV\n",
    "        df = spark.read.csv(\n",
    "                os.path.join(dataset_path, file_name),\n",
    "                header=True,\n",
    "                inferSchema=True,\n",
    "                quote='\"',\n",
    "                escape='\"',\n",
    "                multiLine=True\n",
    "                )\n",
    "\n",
    "\n",
    "        # Clean column names\n",
    "        for old_col in df.columns:\n",
    "            df = df.withColumnRenamed(old_col, old_col.strip().lower())\n",
    "\n",
    "        # Trim string columns\n",
    "        df = trim_string_columns(df)\n",
    "\n",
    "        # Drop duplicates\n",
    "        df = df.dropDuplicates()\n",
    "\n",
    "        # Write to Parquet (single file for readability)\n",
    "        df.coalesce(1).write.mode(\"overwrite\").parquet(table_path)\n",
    "\n",
    "        print(f\"Written to {table_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "print(\"\\n Staging layer successfully created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b1f0b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "|           review_id|            order_id|review_score|review_comment_title|review_comment_message|review_creation_date|review_answer_timestamp|\n",
      "+--------------------+--------------------+------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "|8a310b56e4d05a778...|c5a47daec61dfde19...|           4|           Recomendo|  O pedido chegou a...| 2018-05-08 00:00:00|    2018-05-10 17:19:17|\n",
      "|f51a5d398f2374cb2...|dcbec508f4fc19506...|           1|                   6|  Eu não tenho notí...| 2018-05-02 00:00:00|    2018-05-02 11:17:26|\n",
      "|0e2a9cd112e26220a...|668b09e578d6cdac0...|           5|         Excelente!!|  Produto entregue ...| 2018-07-12 00:00:00|    2018-07-13 11:27:33|\n",
      "|7da6b99eedc285b7a...|df78dad0e4ef0211f...|           5|               ótimo|  O PRODUTO FOI ENT...| 2018-07-05 00:00:00|    2018-07-09 15:29:29|\n",
      "|a6bec9ecc09aeccc7...|afef03e9561e4321c...|           5|    Cortina infantil|  Muito linda. Ótim...| 2018-07-10 00:00:00|    2018-07-11 12:09:41|\n",
      "+--------------------+--------------------+------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "staged_df = spark.read.parquet(\n",
    "    r\"C:\\Users\\vasudha.tanniru\\Documents\\GitHub\\data_projects\\retail_data_warehouse\\warehouse\\staging\\order_reviews\"\n",
    ")\n",
    "staged_df.show(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (pythonData)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
